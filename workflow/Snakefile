import pandas as pd
from pathlib import Path
import subprocess
import random
import string

#INPUTS
configfile: "config/config.yaml"
run_sheet = config["runsheet"]


#path to generated sample sheet
samplesheet_output = "outputs/input/samplesheet.csv"

#creates samplesheet of all samples
if not os.path.exists(samplesheet_output):
    subprocess.run(["python", "workflow/scripts/process_samples.py", run_sheet, samplesheet_output], check=True)


#getting pipeline inputs
read_paths = pd.read_csv(samplesheet_output).to_dict(orient="records")
se_read_paths = [rec for rec in read_paths if rec['sequence_type'] == 'single']
pe_read_paths = [rec for rec in read_paths if rec['sequence_type'] == 'paired']
all_runs = set([rec["run_id"] for rec in read_paths])

#getting single and paired runs
se_all_runs = set([rec["run_id"] for rec in se_read_paths])
pe_all_runs = set([rec["run_id"] for rec in pe_read_paths])

# Create mapping of original sample IDs to short IDs that preserve rcbc pattern
def generate_short_id(original_id, existing_ids):
    """Generate a unique short ID that includes rcbc pattern if present"""
    import re

    # Extract rcbc pattern if present
    rcbc_match = re.search(r'rcbc\d+', original_id)

    if rcbc_match:
        # If rcbc pattern found, use it as the short ID
        short_id = rcbc_match.group()
        # Check for collisions and raise error if found
        if short_id in existing_ids:
            raise ValueError(f"Duplicate rcbc ID detected: {short_id} appears in multiple samples")
        return short_id
    else:
        # Fallback to random ID if no rcbc pattern
        while True:
            short_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
            if short_id not in existing_ids:
                return short_id


# Set seed for reproducibility
random.seed(42)

# Get all unique sample IDs
all_original_sample_ids = list(set([rec["sample_id"] for rec in read_paths]))

# Create mapping from original to short IDs
short_id_mapping = {}
used_short_ids = set()
for original_id in all_original_sample_ids:
    short_id = generate_short_id(original_id, used_short_ids)
    short_id_mapping[original_id] = short_id
    used_short_ids.add(short_id)

# Create reverse mapping (short to original)
reverse_id_mapping = {v: k for k, v in short_id_mapping.items()}

print(reverse_id_mapping)

# Update sample maps to use short IDs
sample_map_se = {short_id_mapping[rec["sample_id"]]: rec['fwd'] 
                 for rec in read_paths if rec["sequence_type"] == "single"}
sample_map_pe = {short_id_mapping[rec["sample_id"]]: {'fwd': rec['fwd'], 'rev': rec['rev']} 
                 for rec in read_paths if rec["sequence_type"] == "paired"}

#gets all samples for single end runs (using short IDs)
all_samples_se = {}
for run in se_all_runs: 
    all_samples_se[run] = [short_id_mapping[rec["sample_id"]] 
                           for rec in read_paths if rec["run_id"] == run and rec["sequence_type"] == "single"]
se_sample_ids = [s for r in all_samples_se.keys() for s in all_samples_se[r]]

#gets all samples for paired end runs (using short IDs)
all_samples_pe = {}
for run in pe_all_runs: 
    all_samples_pe[run] = [short_id_mapping[rec["sample_id"]] 
                           for rec in read_paths if rec["run_id"] == run and rec["sequence_type"] == "paired"]
pe_sample_ids = [s for r in all_samples_pe.keys() for s in all_samples_pe[r]]

#getting mapping files associated with each run
all_mapping_files = {rec["run_id"]: rec['mapping_file'] for rec in read_paths}

run_regions = pd.read_csv(run_sheet).set_index("run_id")["region"].to_dict()


rule target:
    input:
        expand("outputs/results/{run}-multiqc_report.html", run = all_runs),
        expand("outputs/results/speciateit_ps-{run}.rds", run = all_runs)

rule create_id_mapping:
    output:
        mapping = "outputs/dada2_processing/{run}_sample_id_mapping.tsv"
    params:
        mapping_dict = lambda wildcards: {short: reverse_id_mapping[short] 
                                         for short in (all_samples_se.get(wildcards.run, []) + 
                                                      all_samples_pe.get(wildcards.run, []))}
    localrule: True
    run:
        import pandas as pd
        df = pd.DataFrame(list(params.mapping_dict.items()), columns=['short_id', 'original_id'])
        df.to_csv(output.mapping, sep='\t', index=False)


include: "rules/processing_se_runs.smk"
include: "rules/processing_pe_runs.smk"


def get_dada2_output(wildcards):
    return "outputs/dada2_processing/results/dada2-se/{}-seqTab.collapsed.RDS".format(wildcards.run) if wildcards.run in se_all_runs else "outputs/dada2_processing/results/dada2-pe/{}-seqTab.nochimeras.RDS".format(wildcards.run)

rule gtdb_assign_taxonomy:
    input:
        seqtab = get_dada2_output,
        assign_taxonomy = config["gtdb_tax_db"],
        assign_species = config["gtdb_species_db"]
    output:
        taxonomy = "outputs/dada2_processing/results/dada2/gtdb_taxonomy-{run}.RDS"
    conda:
        "envs/16s_tools.yaml"
    threads:
        8
    resources:
        cpus_per_task=8, 
        mem_mb=24000,
        runtime="8h",
        partition="short"
    script:
        "scripts/assign_taxonomy.R"

    
def get_mapping(wildcards):
    return all_mapping_files[wildcards.run]

rule making_phyloseq:
    input:
        mapping_file= get_mapping,
        seqtab = get_dada2_output,
        taxonomy = "outputs/dada2_processing/results/dada2/gtdb_taxonomy-{run}.RDS",
        id_mapping = "outputs/dada2_processing/{run}_sample_id_mapping.tsv"
    output:
        ps = "outputs/results/gtdb_ps-{run}.rds"
    params:
        threshold = 0
    conda:
        "envs/16s_tools.yaml"
    resources:
        cpus_per_task=2, 
        mem_mb=4000,
        runtime="8h",
        partition="short"
    script:
        "scripts/making_phyloseq.R"

# not on conda
rule install_tidysq:
    output:
        tidy_install = touch("outputs/flags/tidyq_installed.done")
    conda:
        "envs/16s_tools.yaml"
    resources:
        cpus_per_task=2, 
        mem_mb=1000,
        runtime="2h",
        partition="short"
    shell:
        """
        Rscript -e 'if(!require(tidysq)){{ install.packages("tidysq", repos="https://cloud.r-project.org", quiet = TRUE) }}'
        echo "tidysq successfully installed" > {output.tidy_install}
        """

rule spikein_adjustment:
    input:
        tidy_install = "outputs/flags/tidyq_installed.done",
        ps = "outputs/results/gtdb_ps-{run}.rds",
        allobacillus_fasta = "resources/spike_in_files/D6320.refseq/16S/Allobacillus.halotolerans.16S.fasta",
        imtechella_fasta = "resources/spike_in_files/D6320.refseq/16S/Imtechella.halotolerans.16S.fasta",
        spikein_manual_taxonomy = "resources/spike_in_files/manual_taxa.csv"
    output: 
        spikein_adjusted_ps = "outputs/phyloseq/spikein_adjusted_ps-{run}.rds"
    conda:
        "envs/16s_tools.yaml"
    resources:
        cpus_per_task=2, 
        mem_mb=8000,
        runtime="8h",
        partition="short"
    script:
        "scripts/spikein_ps_adjustment.R"


include: "rules/speciateit_valencia.smk"


rule get_multiqc_inputs:
    input:
        speciateit_ps = "outputs/results/speciateit_ps-{run}.rds"
    output:
        filt_read_counts = "outputs/multiqc/{run}-read_counts.csv",
        ordplot = "outputs/multiqc/{run}-asv_ordination.png",
        barplot = "outputs/multiqc/{run}-abundance_barplots.png"
    conda:
        "envs/16s_tools.yaml"
    params:
        run = lambda wildcards: wildcards.run,
        read_counts = "outputs/dada2_processing/reports",
        low_read_samples = "outputs/logs/too_few_reads.txt"
    resources:
        cpus_per_task=2, 
        mem_mb=4000,
        runtime="8h",
        partition="short"
    script:
        "scripts/get_multiqc_inputs.R"

# generates one per run
rule multiqc:
    input:
        runs = "outputs/results/speciateit_ps-{run}.rds",
        ordplot = "outputs/multiqc/{run}-asv_ordination.png",
        barplot = "outputs/multiqc/{run}-abundance_barplots.png",
        filt_read_counts = "outputs/multiqc/{run}-read_counts.csv"
    output:
        report="outputs/results/{run}-multiqc_report.html"
    params:
        use_input_files_only=True,
        extra="-c config/mqc_config.yaml"
    resources:
        cpus_per_task=2, 
        mem_mb=4000,
        runtime="8h",
        partition="short"
    log:
        "outputs/logs/{run}-multiqc.log",
    wrapper:
        "v5.2.1/bio/multiqc"
